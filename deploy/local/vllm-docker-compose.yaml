version: '3.8'

services:
  chain:
    container_name: chain
    build: 
      context: ../..
      dockerfile: Dockerfile
    ports:
      - 8001:8001
    environment:
      MILVUS_URL: http://milvus-standalone:19530
      VLLM_HOST: http://vllm:8000/v1
    networks:
      - mlek3
  vllm:
    container_name: vllm
    image: vllm/vllm-openai:v0.5.0
    command: --model astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit --dtype float16 --enforce-eager
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - 8000:8000
    ipc: host # Or define shm-size to allow the container to access the host's shared memory (between processes)
    networks:
      - mlek3

networks:
  mlek3:
    external: true